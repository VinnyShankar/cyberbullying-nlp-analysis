{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# MODEL = f'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>category</th>\n",
       "      <th>channel</th>\n",
       "      <th>video_url</th>\n",
       "      <th>video_id</th>\n",
       "      <th>channel_size</th>\n",
       "      <th>identity</th>\n",
       "      <th>score</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8 months? Is that all.</td>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=Wqcy9do6YO0</td>\n",
       "      <td>Wqcy9do6YO0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impressed very effective</td>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=Wqcy9do6YO0</td>\n",
       "      <td>Wqcy9do6YO0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No matter what, you have to use brushes and ag...</td>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=Wqcy9do6YO0</td>\n",
       "      <td>Wqcy9do6YO0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good video BUT, why don't you \"articulate\" tha...</td>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=Wqcy9do6YO0</td>\n",
       "      <td>Wqcy9do6YO0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My wheels werenever done until 215k km, they l...</td>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=Wqcy9do6YO0</td>\n",
       "      <td>Wqcy9do6YO0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment     category  \\\n",
       "0                             8 months? Is that all.  Automobiles   \n",
       "1                           impressed very effective  Automobiles   \n",
       "2  No matter what, you have to use brushes and ag...  Automobiles   \n",
       "3  Good video BUT, why don't you \"articulate\" tha...  Automobiles   \n",
       "4  My wheels werenever done until 215k km, they l...  Automobiles   \n",
       "\n",
       "                                    channel  \\\n",
       "0  https://www.youtube.com/@autofanaticcars   \n",
       "1  https://www.youtube.com/@autofanaticcars   \n",
       "2  https://www.youtube.com/@autofanaticcars   \n",
       "3  https://www.youtube.com/@autofanaticcars   \n",
       "4  https://www.youtube.com/@autofanaticcars   \n",
       "\n",
       "                                     video_url     video_id  channel_size  \\\n",
       "0  https://www.youtube.com/watch?v=Wqcy9do6YO0  Wqcy9do6YO0             0   \n",
       "1  https://www.youtube.com/watch?v=Wqcy9do6YO0  Wqcy9do6YO0             0   \n",
       "2  https://www.youtube.com/watch?v=Wqcy9do6YO0  Wqcy9do6YO0             0   \n",
       "3  https://www.youtube.com/watch?v=Wqcy9do6YO0  Wqcy9do6YO0             0   \n",
       "4  https://www.youtube.com/watch?v=Wqcy9do6YO0  Wqcy9do6YO0             0   \n",
       "\n",
       "   identity score  results  \n",
       "0         1     1        0  \n",
       "1         1     2        0  \n",
       "2         1     1        0  \n",
       "3         1     1        0  \n",
       "4         1     2        0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"280_spam_dropped.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.0\n",
       "1         2.0\n",
       "2         1.0\n",
       "3         1.0\n",
       "4         2.0\n",
       "         ... \n",
       "189757    0.0\n",
       "189758    0.0\n",
       "189759    1.0\n",
       "189760    2.0\n",
       "189761    2.0\n",
       "Name: score, Length: 189762, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m val_encodings \u001b[39m=\u001b[39m tokenizer(val_texts, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \n\u001b[1;32m     24\u001b[0m \u001b[39m# Create label ids per batch\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m train_label_ids \u001b[39m=\u001b[39m [[label_map[label] \u001b[39mfor\u001b[39;49;00m label \u001b[39min\u001b[39;49;00m train_labels]]\n\u001b[1;32m     26\u001b[0m val_label_ids \u001b[39m=\u001b[39m [[label_map[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m val_labels]]\n\u001b[1;32m     28\u001b[0m \u001b[39m# Freeze base model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m val_encodings \u001b[39m=\u001b[39m tokenizer(val_texts, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \n\u001b[1;32m     24\u001b[0m \u001b[39m# Create label ids per batch\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m train_label_ids \u001b[39m=\u001b[39m [[label_map[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m train_labels]]\n\u001b[1;32m     26\u001b[0m val_label_ids \u001b[39m=\u001b[39m [[label_map[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m val_labels]]\n\u001b[1;32m     28\u001b[0m \u001b[39m# Freeze base model\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "\n",
    "# # Train/val split\n",
    "# split_index = int(0.7 * len(df))\n",
    "# train_df = df.iloc[:split_index]\n",
    "# val_df = df.iloc[split_index:]\n",
    "\n",
    "# # Get train and val data\n",
    "# train_texts = train_df['comment'].tolist()\n",
    "# train_labels = train_df['score'].tolist() \n",
    "\n",
    "# val_texts = val_df['comment'].tolist()\n",
    "# val_labels = val_df['score'].tolist()\n",
    "\n",
    "# # Encode data\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True) \n",
    "\n",
    "# # Create label ids per batch\n",
    "# train_label_ids = [[label_map[label] for label in train_labels]]\n",
    "# val_label_ids = [[label_map[label] for label in val_labels]]\n",
    "\n",
    "# # Freeze base model\n",
    "# for param in model.roberta.parameters():\n",
    "#   param.requires_grad = False\n",
    "  \n",
    "# # Training loop\n",
    "# optimizer = transformers.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(3):\n",
    "\n",
    "#   model.train()\n",
    "#   for batch, labels in zip(train_encodings, train_label_ids):\n",
    "  \n",
    "#     outputs = model(batch, labels=labels)\n",
    "#     loss = outputs[0]\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "  \n",
    "#   model.eval()\n",
    "  \n",
    "#   val_outputs = model(val_encodings, val_label_ids)\n",
    "  \n",
    "#   print(f\"Epoch: {epoch+1}, Loss: {val_outputs.loss}, Accuracy: {val_outputs.accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\") \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Append texts and corresponding labels\n",
    "# train_texts = []\n",
    "# train_labels = [] \n",
    "\n",
    "\n",
    "# With 70/30 split\n",
    "split_index = int(0.7 * len(df))\n",
    "\n",
    "# train_df = df.iloc[:split_index]\n",
    "# val_df = df.iloc[split_index:]\n",
    "\n",
    "# Get train and validation data\n",
    "train_df = df.iloc[:split_index]\n",
    "val_df = df.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56929, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_con = pd.to_numeric(df[\"score\"])\n",
    "df = df[df[\"score\"]!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m val_labels \u001b[39m=\u001b[39m val_df[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      7\u001b[0m \u001b[39m# Encode data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_encodings \u001b[39m=\u001b[39m tokenizer(train_texts, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m      9\u001b[0m val_encodings \u001b[39m=\u001b[39m tokenizer(val_texts, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m batch_labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2577\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2575\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2576\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2577\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2578\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2579\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2663\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2659\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2660\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2661\u001b[0m         )\n\u001b[1;32m   2662\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2663\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2664\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2665\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2666\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2667\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2668\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2669\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2670\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2671\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2672\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2673\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2674\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2675\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2676\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2677\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2678\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2679\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2680\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2681\u001b[0m     )\n\u001b[1;32m   2682\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2683\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2684\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2685\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2702\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2844\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2845\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2846\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2847\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2851\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2852\u001b[0m )\n\u001b[0;32m-> 2854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   2855\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2856\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2857\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2858\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2859\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2860\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2861\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2862\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2863\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2864\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2865\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2866\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2867\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2868\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2869\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2870\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2871\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2872\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/tokenization_utils.py:733\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m--> 733\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(ids)\n\u001b[1;32m    734\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    735\u001b[0m input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/tokenization_utils.py:701\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    700\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 701\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    702\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[1;32m    703\u001b[0m     \u001b[39mif\u001b[39;00m is_split_into_words:\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/tokenization_utils.py:579\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    577\u001b[0m ids \u001b[39m=\u001b[39m []\n\u001b[1;32m    578\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n\u001b[0;32m--> 579\u001b[0m     ids\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id_with_added_voc(token))\n\u001b[1;32m    580\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/tokenization_utils.py:588\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder:\n\u001b[1;32m    587\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m--> 588\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id(token)\n",
      "File \u001b[0;32m~/anaconda3/envs/Minecraft/lib/python3.11/site-packages/transformers/models/roberta/tokenization_roberta.py:305\u001b[0m, in \u001b[0;36mRobertaTokenizer._convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_token_to_id\u001b[39m(\u001b[39mself\u001b[39m, token):\n\u001b[1;32m    304\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mget(token, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_token))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_texts = train_df['comment'].tolist()\n",
    "train_labels = train_df['score'].tolist()\n",
    "\n",
    "val_texts = val_df['comment'].tolist()\n",
    "val_labels = val_df['score'].tolist()\n",
    "\n",
    "# Encode data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True) \n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "batch_labels = []\n",
    "\n",
    "\n",
    "# Freeze base model  \n",
    "for param in model.roberta.parameters():\n",
    "  param.requires_grad = False\n",
    "  \n",
    "# Training loop\n",
    "optimizer = tf.optimizers.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "\n",
    "  model.train()\n",
    "  for batch in train_encodings:  \n",
    "    labels = val_df[\"score\"].tolist()\n",
    "    outputs = model(batch, labels=batch_labels)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "  model.eval()\n",
    "  \n",
    "  val_outputs = model(val_encodings, val_labels)\n",
    "\n",
    "  print(f\"Epoch: {epoch+1}, Loss: {val_outputs.loss}, Accuracy: {val_outputs.accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>channel</th>\n",
       "      <th>video_url</th>\n",
       "      <th>video_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>channel_size</th>\n",
       "      <th>identity</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=Wqcy9do6YO0</td>\n",
       "      <td>Wqcy9do6YO0</td>\n",
       "      <td>[Wheel Cleaner is In production and pre-order ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=NyNF0MfQN8s</td>\n",
       "      <td>NyNF0MfQN8s</td>\n",
       "      <td>[I know that on my 1122TST it also matters wha...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@autofanaticcars</td>\n",
       "      <td>https://www.youtube.com/watch?v=x3vzurNYbyM</td>\n",
       "      <td>x3vzurNYbyM</td>\n",
       "      <td>[Can you put it on top of PPF? Thanks, very ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 2, 0, 0, 1, 1, 1, 2, 2, 1, 2, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@AutoAmateur</td>\n",
       "      <td>https://www.youtube.com/watch?v=qysq4YFOwPw</td>\n",
       "      <td>qysq4YFOwPw</td>\n",
       "      <td>[Hey australian Elon Musk can you show a littl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 1, 2, 2, 0, 2, 0, 1, 0, 0, 2, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automobiles</td>\n",
       "      <td>https://www.youtube.com/@AutoAmateur</td>\n",
       "      <td>https://www.youtube.com/watch?v=B8zgCdINaAs</td>\n",
       "      <td>B8zgCdINaAs</td>\n",
       "      <td>[Excellent tutorial video. Congrats! Maybe two...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                   channel  \\\n",
       "0  Automobiles  https://www.youtube.com/@autofanaticcars   \n",
       "1  Automobiles  https://www.youtube.com/@autofanaticcars   \n",
       "2  Automobiles  https://www.youtube.com/@autofanaticcars   \n",
       "3  Automobiles      https://www.youtube.com/@AutoAmateur   \n",
       "4  Automobiles      https://www.youtube.com/@AutoAmateur   \n",
       "\n",
       "                                     video_url     video_id  \\\n",
       "0  https://www.youtube.com/watch?v=Wqcy9do6YO0  Wqcy9do6YO0   \n",
       "1  https://www.youtube.com/watch?v=NyNF0MfQN8s  NyNF0MfQN8s   \n",
       "2  https://www.youtube.com/watch?v=x3vzurNYbyM  x3vzurNYbyM   \n",
       "3  https://www.youtube.com/watch?v=qysq4YFOwPw  qysq4YFOwPw   \n",
       "4  https://www.youtube.com/watch?v=B8zgCdINaAs  B8zgCdINaAs   \n",
       "\n",
       "                                            comments  channel_size  identity  \\\n",
       "0  [Wheel Cleaner is In production and pre-order ...             0         1   \n",
       "1  [I know that on my 1122TST it also matters wha...             0         1   \n",
       "2  [Can you put it on top of PPF? Thanks, very ha...             0         1   \n",
       "3  [Hey australian Elon Musk can you show a littl...             0         1   \n",
       "4  [Excellent tutorial video. Congrats! Maybe two...             0         1   \n",
       "\n",
       "                                              scores  \n",
       "0  [2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, ...  \n",
       "1  [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, ...  \n",
       "2  [1, 0, 1, 2, 0, 0, 1, 1, 1, 2, 2, 1, 2, 0, 1, ...  \n",
       "3  [1, 2, 1, 2, 2, 0, 2, 0, 1, 0, 0, 2, 1, 1, 2, ...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"comments\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"scores\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"this_took_280_minutes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 317 entries, 0 to 316\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   category      317 non-null    object\n",
      " 1   channel       317 non-null    object\n",
      " 2   video_url     317 non-null    object\n",
      " 3   video_id      317 non-null    object\n",
      " 4   comments      317 non-null    object\n",
      " 5   channel_size  317 non-null    int64 \n",
      " 6   identity      317 non-null    int64 \n",
      " 7   scores        317 non-null    object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 30.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
